{"year": ["2021", "2021", "2021", "2021", "2021", "2019", "2021", "2022", "2105", "2021"], "title": ["Transformer in transformer", "[CITATION][C] A survey on visual transformer", "Visual saliency transformer", "The right to talk: An audio-visual transformer approach", "Learning spatio-temporal transformer for visual tracking", "Multimodal transformer with multi-view visual representation for image captioning", "ResT: An efficient transformer for visual recognition", "Contextual transformer networks for visual recognition", "VTNet: Visual transformer network for object goal navigation", "Audio-visual transformer based crowd counting"], "description": ["\u2026 into sub-patches as \u201cvisual words\u201d. Besides the conventional transformer blocks for extracting \nfeatures and attentions of visual sentences, we further embed a sub-transformer into the \u2026", "", "\u2026 on a pure transformer, namely, Visual Saliency Trans\u2026 the transformer to propagate global \ncontexts among image patches. Unlike conventional architectures used in Vision Transformer (\u2026", "\u2026 -Visual Transformer approach, a cross-modality temporal-based computer vision algorithm, \nto highlight main speaker in both audio and visual \u2026 correlations presented in visual and audio \u2026", "\u2026 on the encoder-decoder transformer for visual tracking. The new \u2026 For instance, our \nspatio-temporal transformer tracker sur\u2026 a new transformer architecture dedicated to visual tracking. \u2026", "\u2026 features may fail to cover all objects in the image, leading to insufficient visual \u2026 , we extend \nthe Transformer model for machine translation [14] to a Multimodal Transformer (MT) model for \u2026", "\u2026 vision Transformer, called ResT, that capably served as a general-purpose backbone for \nimage recognition. Unlike existing Transformer methods, which employ standard Transformer \u2026", "\u2026 self-attention and our Contextual Transformer (CoT) block. (a) \u2026 way to enhance \nTransformer-style architecture by exploiting \u2026 of Transformerstyle block, named Contextual \u2026", "\u2026 We observe that as a visual transformer becomes too deep, a transformer may fail to converge \nto an optimal policy. On the other hand, a transformer with a single encoder and decoder \u2026", "\u2026 transformer-based [51] audio-visual multi-task crowd counting network as shown in Fig. 2. It \nconsists of an Audio-Visual Transformer (\u2026 a third run-time audio-visual attended modality that \u2026"], "authors": ["K Han, A Xiao, E Wu, J Guo, C Xu\u2026", "K Han, Y Wang, H Chen, X Chen, J Guo, Z Liu, Y Tang\u2026", "N Liu, N Zhang, K Wan, L Shao\u2026", "TD Truong, CN Duong, HA Pham\u2026", "B Yan, H Peng, J Fu, D Wang\u2026", "J Yu, J Li, Z Yu, Q Huang", "Q Zhang, YB Yang", "Y Li, T Yao, Y Pan, T Mei", "H Du, X Yu, L Zheng", "U Sajid, X Chen, H Sajid, T Kim\u2026"], "link": ["https://proceedings.neurips.cc/paper/2021/hash/854d9fca60b4bd07f9bb215d59ef5561-Abstract.html", "", "http://openaccess.thecvf.com/content/ICCV2021/html/Liu_Visual_Saliency_Transformer_ICCV_2021_paper.html", "http://openaccess.thecvf.com/content/ICCV2021/html/Truong_The_Right_To_Talk_An_Audio-Visual_Transformer_Approach_ICCV_2021_paper.html", "http://openaccess.thecvf.com/content/ICCV2021/html/Yan_Learning_Spatio-Temporal_Transformer_for_Visual_Tracking_ICCV_2021_paper.html", "https://ieeexplore.ieee.org/abstract/document/8869845/", "https://proceedings.neurips.cc/paper/2021/hash/82c2559140b95ccda9c6ca4a8b981f1e-Abstract.html", "https://ieeexplore.ieee.org/abstract/document/9747984/", "https://arxiv.org/abs/2105.09447", "https://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/html/Sajid_Audio-Visual_Transformer_Based_Crowd_Counting_ICCVW_2021_paper.html"], "pdf_link": ["https://proceedings.neurips.cc/paper/2021/file/854d9fca60b4bd07f9bb215d59ef5561-Paper.pdf", null, "http://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Visual_Saliency_Transformer_ICCV_2021_paper.pdf", "https://openaccess.thecvf.com/content/ICCV2021/papers/Truong_The_Right_To_Talk_An_Audio-Visual_Transformer_Approach_ICCV_2021_paper.pdf", "https://openaccess.thecvf.com/content/ICCV2021/papers/Yan_Learning_Spatio-Temporal_Transformer_for_Visual_Tracking_ICCV_2021_paper.pdf", "https://arxiv.org/pdf/1905.07841", "https://proceedings.neurips.cc/paper/2021/file/82c2559140b95ccda9c6ca4a8b981f1e-Paper.pdf", "https://arxiv.org/pdf/2107.12292", "https://arxiv.org/pdf/2105.09447", "https://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/papers/Sajid_Audio-Visual_Transformer_Based_Crowd_Counting_ICCVW_2021_paper.pdf"], "journal_domain": ["proceedings.neurips.cc", null, "openaccess.thecvf.com", "openaccess.thecvf.com", "openaccess.thecvf.com", "ieeexplore.ieee.org", "proceedings.neurips.cc", "ieeexplore.ieee.org", "2105.09447", "openaccess.thecvf.com"], "domain": ["neurips.cc", null, "thecvf.com", "thecvf.com", "thecvf.com", "ieee.org", "neurips.cc", "ieee.org", null, "thecvf.com"], "many_version": ["All 10 versions", "All 2 versions", "All 5 versions", "All 6 versions", "All 8 versions", "All 5 versions", "All 4 versions", "All 6 versions", "All 3 versions", "All 7 versions"]}